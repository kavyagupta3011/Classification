{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36387f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# ==========================================\n",
    "# 2. ADVANCED FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Activity Score\n",
    "    df['activity_score'] = (df['hobby_engagement_level'] + \n",
    "                            df['physical_activity_index'] + \n",
    "                            df['creative_expression_index'])\n",
    "    \n",
    "    # 2. Support Score\n",
    "    df['support_total'] = (df['support_environment_score'] + \n",
    "                           df['external_guidance_usage'] + \n",
    "                           df['upbringing_influence'])\n",
    "    \n",
    "    # 3. Efficiency Ratios (Adding epsilon for safety)\n",
    "    df['efficiency'] = df['consistency_score'] / (df['focus_intensity'] + 1.0)\n",
    "    df['focus_per_support'] = df['focus_intensity'] / (df['support_environment_score'] + 1.0)\n",
    "    \n",
    "    # 4. Age Norms\n",
    "    df['consistency_per_age'] = df['consistency_score'] / df['age_group']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Generating Base Features...\")\n",
    "train_df = create_features(train_df)\n",
    "test_df = create_features(test_df)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PCA-GUIDED CLUSTERING FEATURES\n",
    "# ==========================================\n",
    "print(\"Generating PCA-Cluster Features...\")\n",
    "\n",
    "# Combine for global structure learning (excluding target)\n",
    "train_temp = train_df.drop(['personality_cluster', 'participant_id'], axis=1)\n",
    "test_temp = test_df.drop(['participant_id'], axis=1)\n",
    "full_data = pd.concat([train_temp, test_temp], axis=0, ignore_index=True)\n",
    "\n",
    "# Select columns for structure\n",
    "cluster_cols = ['focus_intensity', 'consistency_score', 'efficiency', \n",
    "                'activity_score', 'support_total', 'focus_per_support']\n",
    "\n",
    "# Scale for PCA\n",
    "scaler_pca = StandardScaler()\n",
    "full_scaled = scaler_pca.fit_transform(full_data[cluster_cols])\n",
    "\n",
    "# PCA (Keep 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "full_pca = pca.fit_transform(full_scaled)\n",
    "\n",
    "# Cluster 5 (Target aligned)\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=20)\n",
    "full_data['cluster_pca_5'] = kmeans_5.fit_predict(full_pca)\n",
    "\n",
    "# Cluster 8 (Sub-types)\n",
    "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=20)\n",
    "full_data['cluster_pca_8'] = kmeans_8.fit_predict(full_pca)\n",
    "\n",
    "# Split back to Train/Test\n",
    "train_df['cluster_pca_5'] = full_data.iloc[:len(train_df)]['cluster_pca_5'].values\n",
    "train_df['cluster_pca_8'] = full_data.iloc[:len(train_df)]['cluster_pca_8'].values\n",
    "test_df['cluster_pca_5'] = full_data.iloc[len(train_df):]['cluster_pca_5'].values\n",
    "test_df['cluster_pca_8'] = full_data.iloc[len(train_df):]['cluster_pca_8'].values\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPROCESSING\n",
    "# ==========================================\n",
    "X = train_df.drop(['participant_id', 'personality_cluster'], axis=1)\n",
    "y = train_df['personality_cluster']\n",
    "X_test = test_df.drop(['participant_id'], axis=1)\n",
    "\n",
    "# Encode Target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Identify Feature Groups\n",
    "categorical_cols = ['cultural_background', 'cluster_pca_5', 'cluster_pca_8']\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "# Build Transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing Data...\")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 5. NEURAL NETWORK ENSEMBLING (K-FOLD)\n",
    "# ==========================================\n",
    "# Instead of one split, we use 5 splits and average the results.\n",
    "N_FOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Store test predictions (probabilities) to average later\n",
    "test_probs_sum = np.zeros((X_test_processed.shape[0], num_classes))\n",
    "oof_preds = np.zeros(X_processed.shape[0]) # Out-of-fold predictions\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"\\nStarting {N_FOLDS}-Fold Cross-Validation Ensemble...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_processed, y_enc)):\n",
    "    # 1. Split Data\n",
    "    X_train_fold, X_val_fold = X_processed[train_idx], X_processed[val_idx]\n",
    "    y_train_fold, y_val_fold = y_enc[train_idx], y_enc[val_idx]\n",
    "    \n",
    "    # 2. Compute Class Weights for this fold\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', classes=np.unique(y_enc), y=y_train_fold\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # 3. Define Model Architecture (Fresh for each fold)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train_fold.shape[1],)),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # 4. Train with Early Stopping\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True, verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Training Fold {fold+1}...\")\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        epochs=60,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0 # Silent training\n",
    "    )\n",
    "    \n",
    "    # 5. Evaluate on Validation\n",
    "    val_probs = model.predict(X_val_fold, verbose=0)\n",
    "    val_pred_labels = np.argmax(val_probs, axis=1)\n",
    "    score = f1_score(y_val_fold, val_pred_labels, average='macro')\n",
    "    cv_scores.append(score)\n",
    "    print(f\"  Fold {fold+1} F1 Score: {score:.4f}\")\n",
    "    \n",
    "    # 6. Predict on Test (Accumulate Probabilities)\n",
    "    test_probs_sum += model.predict(X_test_processed, verbose=0)\n",
    "\n",
    "# ==========================================\n",
    "# 6. FINAL PREDICTION\n",
    "# ==========================================\n",
    "print(f\"\\nAverage CV F1 Score: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# Average the probabilities across all folds\n",
    "avg_test_probs = test_probs_sum / N_FOLDS\n",
    "final_preds = np.argmax(avg_test_probs, axis=1)\n",
    "final_labels = le.inverse_transform(final_preds)\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test_df['participant_id'],\n",
    "    'personality_cluster': final_labels\n",
    "})\n",
    "filename = 'submission_nn_ensemble_advanced.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"Ensemble Submission Saved: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
