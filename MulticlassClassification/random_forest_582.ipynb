{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be085d-5e58-44f9-858c-6c29c4ae65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "Generating Structure Features...\n",
      "Preprocessing Data...\n",
      "Initializing Forest Models...\n",
      "Training Forest Stack... (This combines 3200 trees)\n",
      "Generating Predictions...\n",
      "SUCCESS! Forest-Based Submission Saved: submission_forest_stack.csv\n",
      "   participant_id personality_cluster\n",
      "0            1005           Cluster_E\n",
      "1             197           Cluster_B\n",
      "2            2343           Cluster_D\n",
      "3            1709           Cluster_A\n",
      "4             436           Cluster_E\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PowerTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Activity Score\n",
    "    df['activity_score'] = (df['hobby_engagement_level'] + \n",
    "                            df['physical_activity_index'] + \n",
    "                            df['creative_expression_index'])\n",
    "    \n",
    "    # 2. Support Score\n",
    "    df['support_total'] = (df['support_environment_score'] + \n",
    "                           df['external_guidance_usage'] + \n",
    "                           df['upbringing_influence'])\n",
    "    \n",
    "    # 3. Efficiency Ratios\n",
    "    df['efficiency'] = df['consistency_score'] / (df['focus_intensity'] + 1.0)\n",
    "    df['focus_per_support'] = df['focus_intensity'] / (df['support_environment_score'] + 1.0)\n",
    "    \n",
    "    # 4. Age Norms\n",
    "    df['consistency_per_age'] = df['consistency_score'] / df['age_group']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "train_eng = create_features(train)\n",
    "test_eng = create_features(test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PCA-GUIDED CLUSTERING\n",
    "# ==========================================\n",
    "print(\"Generating Structure Features...\")\n",
    "\n",
    "full_data = pd.concat([train_eng.drop('personality_cluster', axis=1), test_eng], axis=0, ignore_index=True)\n",
    "cluster_cols = ['focus_intensity', 'consistency_score', 'efficiency', 'activity_score', 'support_total', 'focus_per_support']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_data[cluster_cols])\n",
    "\n",
    "# PCA to remove noise\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "full_pca = pca.fit_transform(full_scaled)\n",
    "\n",
    "# Cluster 5 (Target aligned)\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_5'] = kmeans_5.fit_predict(full_pca)\n",
    "\n",
    "# Cluster 8 (Sub-types)\n",
    "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_8'] = kmeans_8.fit_predict(full_pca)\n",
    "\n",
    "# Split back\n",
    "train_eng['cluster_pca_5'] = full_data.iloc[:len(train)]['cluster_pca_5'].values\n",
    "train_eng['cluster_pca_8'] = full_data.iloc[:len(train)]['cluster_pca_8'].values\n",
    "test_eng['cluster_pca_5'] = full_data.iloc[len(train):]['cluster_pca_5'].values\n",
    "test_eng['cluster_pca_8'] = full_data.iloc[len(train):]['cluster_pca_8'].values\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPROCESSING\n",
    "# ==========================================\n",
    "target_col = 'personality_cluster'\n",
    "X = train_eng.drop([target_col, 'participant_id'], axis=1)\n",
    "y = train_eng[target_col]\n",
    "X_test = test_eng.drop(['participant_id'], axis=1)\n",
    "\n",
    "nominal_cols = ['cultural_background', 'cluster_pca_5', 'cluster_pca_8']\n",
    "numeric_cols = [c for c in X.columns if c not in nominal_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('power', PowerTransformer(method='yeo-johnson')) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, nominal_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing Data...\")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# ==========================================\n",
    "# 5. THE \"QUAD-FOREST\" STACK\n",
    "# ==========================================\n",
    "print(\"Initializing Forest Models...\")\n",
    "\n",
    "# Forest 1: Standard Random Forest (Gini)\n",
    "rf_gini = RandomForestClassifier(\n",
    "    n_estimators=800, \n",
    "    criterion='gini', # Standard math\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Forest 2: Entropy Random Forest (Information Gain)\n",
    "# \"Thinks\" differently about splits\n",
    "rf_entropy = RandomForestClassifier(\n",
    "    n_estimators=800, \n",
    "    criterion='entropy', # Different math\n",
    "    max_depth=10, \n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Forest 3: Extra Trees (Gini)\n",
    "# More random -> Reduces overfitting\n",
    "et_gini = ExtraTreesClassifier(\n",
    "    n_estimators=800,\n",
    "    criterion='gini',\n",
    "    max_depth=12, # ET can be deeper than RF without overfitting\n",
    "    bootstrap=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Forest 4: Extra Trees (Entropy)\n",
    "et_entropy = ExtraTreesClassifier(\n",
    "    n_estimators=800,\n",
    "    criterion='entropy',\n",
    "    max_depth=12,\n",
    "    bootstrap=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Meta-Learner: Logistic Regression\n",
    "# It learns which Forest is right for which sample\n",
    "meta_learner = LogisticRegression(class_weight='balanced', C=0.5, max_iter=2000)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf_g', rf_gini),\n",
    "        ('rf_e', rf_entropy),\n",
    "        ('et_g', et_gini),\n",
    "        ('et_e', et_entropy)\n",
    "    ],\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 6. TRAIN & PREDICT\n",
    "# ==========================================\n",
    "print(\"Training Forest Stack... (This combines 3200 trees)\")\n",
    "stacking_clf.fit(X_processed, y_encoded)\n",
    "\n",
    "print(\"Generating Predictions...\")\n",
    "y_pred_encoded = stacking_clf.predict(X_test_processed)\n",
    "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# ==========================================\n",
    "# 7. SUBMISSION\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test['participant_id'],\n",
    "    'personality_cluster': y_pred_labels\n",
    "})\n",
    "\n",
    "filename = 'submission_forest_stack.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"SUCCESS! Forest-Based Submission Saved: {filename}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27ab4d-dc92-48b3-993d-5c641eb119c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
