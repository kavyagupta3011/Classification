{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347cf83-1eff-4467-9477-92ceb84fcf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission created with shape: (479, 2)\n",
      "Unique predictions: ['Cluster_E' 'Cluster_C' 'Cluster_D' 'Cluster_B' 'Cluster_A']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# 1. LOAD DATA\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Total Activity Score: Sum of the binary/ordinal activity indicators\n",
    "    df['total_activity_score'] = (df['hobby_engagement_level'] + \n",
    "                                  df['physical_activity_index'] + \n",
    "                                  df['creative_expression_index'])\n",
    "    \n",
    "    # 2. Support-Guidance Interaction: High support AND usage of guidance\n",
    "    df['support_x_guidance'] = df['support_environment_score'] * df['external_guidance_usage']\n",
    "    \n",
    "    # --- Additional Density-Helping Features ---\n",
    "    # 3. Focus x Consistency: Differentiates \"scattered\" vs \"disciplined\" behavior\n",
    "    # This is crucial for separating personality clusters\n",
    "    df['focus_x_consistency'] = df['focus_intensity'] * df['consistency_score']\n",
    "    \n",
    "    # 4. Log Transform Continuous Variables\n",
    "    # DBSCAN uses Euclidean distance. Large raw values can dominate the distance.\n",
    "    # Log transform compresses the range, making clusters spherical and easier to find.\n",
    "    # We add +1 to avoid log(0)\n",
    "    df['log_focus'] = np.log1p(df['focus_intensity'])\n",
    "    df['log_consistency'] = np.log1p(df['consistency_score'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_fe = create_features(train_df)\n",
    "test_fe = create_features(test_df)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PREPARATION & PREPROCESSING\n",
    "# ==========================================\n",
    "# Separate Target\n",
    "X_train = train_fe.drop(['participant_id', 'personality_cluster'], axis=1)\n",
    "y_train = train_fe['personality_cluster']\n",
    "X_test = test_fe.drop(['participant_id'], axis=1)\n",
    "\n",
    "# Combine for Transductive Clustering (Clustering on the whole geometry usually helps DBSCAN)\n",
    "X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# Define Columns\n",
    "categorical_cols = ['cultural_background'] # usually the main non-ordinal category\n",
    "# Select numerical columns (excluding the raw ones we log-transformed if we want to reduce noise, \n",
    "# but keeping them is okay if Scaled. Let's drop raw to reduce correlation)\n",
    "numerical_cols = [c for c in X_combined.columns if c not in categorical_cols \n",
    "                  and c not in ['focus_intensity', 'consistency_score']]\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "# We use RobustScaler because DBSCAN is sensitive to outliers. \n",
    "# RobustScaler scales based on percentiles, not min/max or mean/std.\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', RobustScaler(), numerical_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "])\n",
    "\n",
    "X_combined_scaled = preprocessor.fit_transform(X_combined)\n",
    "\n",
    "# ==========================================\n",
    "# 4. RUN DBSCAN\n",
    "# ==========================================\n",
    "db = DBSCAN(eps=1.5, min_samples=5, metric='euclidean', n_jobs=-1)\n",
    "cluster_labels = db.fit_predict(X_combined_scaled)\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAP CLUSTERS TO LABELS\n",
    "# ==========================================\n",
    "train_size = len(X_train)\n",
    "train_cluster_labels = cluster_labels[:train_size]\n",
    "test_cluster_labels = cluster_labels[train_size:]\n",
    "\n",
    "# Strategy:\n",
    "# 1. For every cluster ID found, calculate the most frequent true label in the training set.\n",
    "# 2. Assign that label to all test points in that cluster.\n",
    "# 3. If a test point is \"Noise\" (-1), use KNN to find the nearest labeled neighbor.\n",
    "\n",
    "cluster_map = {}\n",
    "y_train_arr = y_train.values\n",
    "\n",
    "# Find majority label for each cluster\n",
    "unique_clusters = set(train_cluster_labels)\n",
    "for cid in unique_clusters:\n",
    "    if cid == -1: continue # Skip noise for now\n",
    "    \n",
    "    indices = np.where(train_cluster_labels == cid)[0]\n",
    "    true_labels_in_cluster = y_train_arr[indices]\n",
    "    \n",
    "    if len(true_labels_in_cluster) > 0:\n",
    "        most_common = Counter(true_labels_in_cluster).most_common(1)[0][0]\n",
    "        cluster_map[cid] = most_common\n",
    "\n",
    "# Prepare Fallback for Noise (-1) or Empty Clusters\n",
    "# We fit a simple KNN on the TRAIN data to predict labels for \"Noise\" points in Test\n",
    "knn_filler = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_filler.fit(X_combined_scaled[:train_size], y_train)\n",
    "\n",
    "# Generate Predictions\n",
    "final_preds = []\n",
    "\n",
    "# Get scaled test portion for KNN lookup\n",
    "X_test_scaled = X_combined_scaled[train_size:]\n",
    "\n",
    "for i, cluster_id in enumerate(test_cluster_labels):\n",
    "    # Case A: The point belongs to a valid cluster we mapped\n",
    "    if cluster_id != -1 and cluster_id in cluster_map:\n",
    "        final_preds.append(cluster_map[cluster_id])\n",
    "        \n",
    "    # Case B: The point is Noise (-1) OR belongs to a cluster with no training data\n",
    "    else:\n",
    "        # Use KNN to find the closest labeled point\n",
    "        # Reshape is needed for single prediction\n",
    "        current_point = X_test_scaled[i].reshape(1, -1)\n",
    "        predicted_label = knn_filler.predict(current_point)[0]\n",
    "        final_preds.append(predicted_label)\n",
    "\n",
    "# ==========================================\n",
    "# 6. SUBMISSION\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test_df['participant_id'],\n",
    "    'personality_cluster': final_preds\n",
    "})\n",
    "\n",
    "print(\"Submission created with shape:\", submission.shape)\n",
    "print(\"Unique predictions:\", submission['personality_cluster'].unique())\n",
    "submission.to_csv('submission_dbscan_improved.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46ff9e-5d0a-42be-b614-73abc9762852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
