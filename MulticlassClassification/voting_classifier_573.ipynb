{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d94e28-46e2-4c3f-9728-321fd1234591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering & Cleaning...\n",
      "Generating Structure Features...\n",
      "Preprocessing Data...\n",
      "Initializing Optimized Boosting Models...\n",
      "Training Boosting Ensemble...\n",
      "Generating Predictions...\n",
      "SUCCESS! Optimized XGBoost-style Submission Saved: submission_optimized_xgboost.csv\n",
      "   participant_id personality_cluster\n",
      "0            1005           Cluster_E\n",
      "1             197           Cluster_C\n",
      "2            2343           Cluster_E\n",
      "3            1709           Cluster_B\n",
      "4             436           Cluster_E\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PowerTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# The Boosting Models\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # These columns had near-zero importance in previous runs\n",
    "    df = df.drop(columns=['altruism_score', 'identity_code'], errors='ignore')\n",
    "    \n",
    "    # 2. Activity Score\n",
    "    df['activity_score'] = (df['hobby_engagement_level'] + \n",
    "                            df['physical_activity_index'] + \n",
    "                            df['creative_expression_index'])\n",
    "    \n",
    "    # 3. Support Score\n",
    "    df['support_total'] = (df['support_environment_score'] + \n",
    "                           df['external_guidance_usage'] + \n",
    "                           df['upbringing_influence'])\n",
    "    \n",
    "    # 4. Efficiency Ratios\n",
    "    df['efficiency'] = df['consistency_score'] / (df['focus_intensity'] + 1.0)\n",
    "    df['focus_per_support'] = df['focus_intensity'] / (df['support_environment_score'] + 1.0)\n",
    "    \n",
    "    # 5. Age Norms\n",
    "    df['consistency_per_age'] = df['consistency_score'] / df['age_group']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature Engineering & Cleaning...\")\n",
    "train_eng = create_features(train)\n",
    "test_eng = create_features(test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PCA-GUIDED CLUSTERING (The Secret Sauce)\n",
    "# ==========================================\n",
    "print(\"Generating Structure Features...\")\n",
    "\n",
    "full_data = pd.concat([train_eng.drop('personality_cluster', axis=1), test_eng], axis=0, ignore_index=True)\n",
    "# We exclude the dropped columns automatically\n",
    "cluster_cols = ['focus_intensity', 'consistency_score', 'efficiency', 'activity_score', 'support_total', 'focus_per_support']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_data[cluster_cols])\n",
    "\n",
    "# PCA (95% Variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "full_pca = pca.fit_transform(full_scaled)\n",
    "\n",
    "# Cluster 5 & 8\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_5'] = kmeans_5.fit_predict(full_pca)\n",
    "\n",
    "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_8'] = kmeans_8.fit_predict(full_pca)\n",
    "\n",
    "# Split back\n",
    "train_eng['cluster_pca_5'] = full_data.iloc[:len(train)]['cluster_pca_5'].values\n",
    "train_eng['cluster_pca_8'] = full_data.iloc[:len(train)]['cluster_pca_8'].values\n",
    "test_eng['cluster_pca_5'] = full_data.iloc[len(train):]['cluster_pca_5'].values\n",
    "test_eng['cluster_pca_8'] = full_data.iloc[len(train):]['cluster_pca_8'].values\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPROCESSING\n",
    "# ==========================================\n",
    "target_col = 'personality_cluster'\n",
    "X = train_eng.drop([target_col, 'participant_id'], axis=1)\n",
    "y = train_eng[target_col]\n",
    "X_test = test_eng.drop(['participant_id'], axis=1)\n",
    "\n",
    "nominal_cols = ['cultural_background', 'cluster_pca_5', 'cluster_pca_8']\n",
    "numeric_cols = [c for c in X.columns if c not in nominal_cols]\n",
    "\n",
    "# PowerTransformer is crucial for Boosting to handle outliers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('power', PowerTransformer(method='yeo-johnson')) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, nominal_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing Data...\")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# ==========================================\n",
    "# 5. THE \"TRIPLE-BOOST\" ENSEMBLE\n",
    "# ==========================================\n",
    "print(\"Initializing Optimized Boosting Models...\")\n",
    "\n",
    "# Model A: \"The Tank\" (High Regularization)\n",
    "# Prevents overfitting by penalizing complex rules (L2=10)\n",
    "hgb_tank = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.03, \n",
    "    max_iter=1000, \n",
    "    max_depth=6, \n",
    "    l2_regularization=10.0, \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model B: \"The Sniper\"(Precision)\n",
    "hgb_sniper = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.01, \n",
    "    max_iter=2000, \n",
    "    max_depth=10, \n",
    "    l2_regularization=5.0, \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model C: \"The Scout\" (Broad Strokes)\n",
    "hgb_scout = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05, \n",
    "    max_iter=800, \n",
    "    max_depth=4, \n",
    "    l2_regularization=5.0, \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Voting: Soft averaging of probabilities\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('tank', hgb_tank),\n",
    "        ('sniper', hgb_sniper),\n",
    "        ('scout', hgb_scout)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[2, 3, 1] # Giving highest trust to the \"Sniper\" (Deep/Slow) model\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 6. TRAIN & PREDICT\n",
    "# ==========================================\n",
    "print(\"Training Boosting Ensemble...\")\n",
    "voting_clf.fit(X_processed, y_encoded)\n",
    "\n",
    "print(\"Generating Predictions...\")\n",
    "y_pred_encoded = voting_clf.predict(X_test_processed)\n",
    "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# ==========================================\n",
    "# 7. SUBMISSION\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test['participant_id'],\n",
    "    'personality_cluster': y_pred_labels\n",
    "})\n",
    "\n",
    "filename = 'submission_optimized_xgboost.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"SUCCESS! Optimized XGBoost-style Submission Saved: {filename}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea6058-5c2a-4891-8e70-14c99a60693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
