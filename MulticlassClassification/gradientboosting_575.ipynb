{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ab1be-ae36-44e2-b926-5d53815b842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "Generating Structure Features...\n",
      "Preprocessing Data...\n",
      "Training Single Optimized XGBoost-style Model...\n",
      "Generating Predictions...\n",
      "SUCCESS! Submission Saved: submission_pure_xgboost_optimised.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PowerTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Drop Noise\n",
    "    df = df.drop(columns=['altruism_score', 'identity_code'], errors='ignore')\n",
    "    \n",
    "    # 2. Activity Score\n",
    "    df['activity_score'] = (df['hobby_engagement_level'] + \n",
    "                            df['physical_activity_index'] + \n",
    "                            df['creative_expression_index'])\n",
    "    \n",
    "    # 3. Support Score\n",
    "    df['support_total'] = (df['support_environment_score'] + \n",
    "                           df['external_guidance_usage'] + \n",
    "                           df['upbringing_influence'])\n",
    "    \n",
    "    # 4. Efficiency Ratios\n",
    "    df['efficiency'] = df['consistency_score'] / (df['focus_intensity'] + 1.0)\n",
    "    df['focus_per_support'] = df['focus_intensity'] / (df['support_environment_score'] + 1.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "train_eng = create_features(train)\n",
    "test_eng = create_features(test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PCA-GUIDED CLUSTERING\n",
    "# ==========================================\n",
    "print(\"Generating Structure Features...\")\n",
    "full_data = pd.concat([train_eng.drop('personality_cluster', axis=1), test_eng], axis=0, ignore_index=True)\n",
    "\n",
    "# Select numeric columns for structure finding\n",
    "cluster_cols = ['focus_intensity', 'consistency_score', 'efficiency', 'activity_score', 'support_total', 'focus_per_support']\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_data[cluster_cols])\n",
    "\n",
    "# PCA (95% Variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "full_pca = pca.fit_transform(full_scaled)\n",
    "\n",
    "# Cluster 5 (Target aligned)\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_5'] = kmeans_5.fit_predict(full_pca)\n",
    "\n",
    "# Cluster 8 (Sub-types)\n",
    "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_8'] = kmeans_8.fit_predict(full_pca)\n",
    "\n",
    "# Split back\n",
    "train_eng['cluster_pca_5'] = full_data.iloc[:len(train)]['cluster_pca_5'].values\n",
    "train_eng['cluster_pca_8'] = full_data.iloc[:len(train)]['cluster_pca_8'].values\n",
    "test_eng['cluster_pca_5'] = full_data.iloc[len(train):]['cluster_pca_5'].values\n",
    "test_eng['cluster_pca_8'] = full_data.iloc[len(train):]['cluster_pca_8'].values\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPROCESSING\n",
    "# ==========================================\n",
    "target_col = 'personality_cluster'\n",
    "X = train_eng.drop([target_col, 'participant_id'], axis=1)\n",
    "y = train_eng[target_col]\n",
    "X_test = test_eng.drop(['participant_id'], axis=1)\n",
    "\n",
    "nominal_cols = ['cultural_background', 'cluster_pca_5', 'cluster_pca_8']\n",
    "numeric_cols = [c for c in X.columns if c not in nominal_cols]\n",
    "\n",
    "# Numeric Pipeline: PowerTransformer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('power', PowerTransformer(method='yeo-johnson')) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, nominal_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing Data...\")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# ==========================================\n",
    "# 5. SINGLE OPTIMIZED GRADIENT BOOSTING MODEL\n",
    "# ==========================================\n",
    "print(\"Training Single Optimized XGBoost-style Model...\")\n",
    "\n",
    "# This configuration mimics a highly tuned XGBoost model\n",
    "# - learning_rate=0.01: Very slow learning for maximum precision\n",
    "# - max_iter=2000: Needed because of the slow learning rate\n",
    "# - max_depth=8: Allows learning complex interactions\n",
    "# - l2_regularization=10.0: Strong regularization to prevent overfitting\n",
    "# - early_stopping=True: Automatically stops if it stops improving (prevents overtraining)\n",
    "\n",
    "model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.01, \n",
    "    max_iter=2000, \n",
    "    max_depth=8, \n",
    "    l2_regularization=10.0, \n",
    "    max_leaf_nodes=40, \n",
    "    class_weight='balanced', \n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_processed, y_encoded)\n",
    "\n",
    "# ==========================================\n",
    "# 6. SUBMISSION\n",
    "# ==========================================\n",
    "print(\"Generating Predictions...\")\n",
    "y_pred_encoded = model.predict(X_test_processed)\n",
    "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test['participant_id'],\n",
    "    'personality_cluster': y_pred_labels\n",
    "})\n",
    "\n",
    "filename = 'submission_pure_xgboost_optimised.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"SUCCESS! Submission Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb35a8f-1fab-4e66-81a6-17698f60019e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
