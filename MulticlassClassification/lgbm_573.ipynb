{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026575c3-0916-4fe2-a0d4-b7ff43052a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering...\n",
      "Generating PCA-Cluster Features...\n",
      "Training LightGBM Ensemble...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[473]\ttraining's multi_logloss: 0.469909\tvalid_1's multi_logloss: 0.777312\n",
      "Fold 1/10 | F1 Macro: 0.53454\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's multi_logloss: 0.522641\tvalid_1's multi_logloss: 0.745753\n",
      "Fold 2/10 | F1 Macro: 0.62167\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's multi_logloss: 0.504518\tvalid_1's multi_logloss: 0.732621\n",
      "Fold 3/10 | F1 Macro: 0.57009\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[272]\ttraining's multi_logloss: 0.578302\tvalid_1's multi_logloss: 0.760933\n",
      "Fold 4/10 | F1 Macro: 0.52144\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's multi_logloss: 0.490074\tvalid_1's multi_logloss: 0.774052\n",
      "Fold 5/10 | F1 Macro: 0.50703\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[489]\ttraining's multi_logloss: 0.46428\tvalid_1's multi_logloss: 0.73221\n",
      "Fold 6/10 | F1 Macro: 0.62038\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[289]\ttraining's multi_logloss: 0.562558\tvalid_1's multi_logloss: 0.764568\n",
      "Fold 7/10 | F1 Macro: 0.56099\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[450]\ttraining's multi_logloss: 0.479618\tvalid_1's multi_logloss: 0.767216\n",
      "Fold 8/10 | F1 Macro: 0.59081\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[340]\ttraining's multi_logloss: 0.531306\tvalid_1's multi_logloss: 0.80539\n",
      "Fold 9/10 | F1 Macro: 0.47677\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttraining's multi_logloss: 0.450309\tvalid_1's multi_logloss: 0.709683\n",
      "Fold 10/10 | F1 Macro: 0.62032\n",
      "\n",
      "Average CV F1: 0.56240 +/- 0.04894\n",
      "Submission saved to submission_pure_lgbm.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Activity Score\n",
    "    df['activity_score'] = (df['hobby_engagement_level'] + \n",
    "                            df['physical_activity_index'] + \n",
    "                            df['creative_expression_index'])\n",
    "    \n",
    "    # 2. Support Score\n",
    "    df['support_total'] = (df['support_environment_score'] + \n",
    "                           df['external_guidance_usage'] + \n",
    "                           df['upbringing_influence'])\n",
    "    \n",
    "    # 3. Efficiency Ratios\n",
    "    df['efficiency'] = df['consistency_score'] / (df['focus_intensity'] + 1.0)\n",
    "    df['focus_per_support'] = df['focus_intensity'] / (df['support_environment_score'] + 1.0)\n",
    "    \n",
    "    # 4. Age Norms\n",
    "    df['consistency_per_age'] = df['consistency_score'] / df['age_group']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "train_eng = create_features(train)\n",
    "test_eng = create_features(test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PCA-GUIDED CLUSTERING (Crucial Step)\n",
    "# ==========================================\n",
    "print(\"Generating PCA-Cluster Features...\")\n",
    "\n",
    "# Combine for structure learning\n",
    "full_data = pd.concat([train_eng.drop('personality_cluster', axis=1), test_eng], axis=0, ignore_index=True)\n",
    "\n",
    "# Select columns that define structure\n",
    "cluster_cols = ['focus_intensity', 'consistency_score', 'efficiency', 'activity_score', 'support_total', 'focus_per_support']\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_data[cluster_cols])\n",
    "\n",
    "# PCA (Keep 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "full_pca = pca.fit_transform(full_scaled)\n",
    "\n",
    "# Cluster Features\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_5'] = kmeans_5.fit_predict(full_pca)\n",
    "\n",
    "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=50)\n",
    "full_data['cluster_pca_8'] = kmeans_8.fit_predict(full_pca)\n",
    "\n",
    "# Split back\n",
    "train_eng['cluster_pca_5'] = full_data.iloc[:len(train)]['cluster_pca_5'].values\n",
    "train_eng['cluster_pca_8'] = full_data.iloc[:len(train)]['cluster_pca_8'].values\n",
    "test_eng['cluster_pca_5'] = full_data.iloc[len(train):]['cluster_pca_5'].values\n",
    "test_eng['cluster_pca_8'] = full_data.iloc[len(train):]['cluster_pca_8'].values\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPARE DATA FOR LIGHTGBM\n",
    "# ==========================================\n",
    "target_col = 'personality_cluster'\n",
    "drop_cols = ['participant_id', target_col]\n",
    "\n",
    "X = train_eng.drop(drop_cols, axis=1)\n",
    "y = train_eng[target_col]\n",
    "X_test = test_eng.drop(['participant_id'], axis=1)\n",
    "\n",
    "# Encode Target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identify Categorical Features\n",
    "# LightGBM handles these natively and often better than OneHot\n",
    "cat_cols = ['cultural_background', 'cluster_pca_5', 'cluster_pca_8', 'identity_code']\n",
    "# Ensure they are category type for LGBM\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# ==========================================\n",
    "# 5. STRATIFIED LIGHTGBM ENSEMBLE\n",
    "# ==========================================\n",
    "print(\"Training LightGBM Ensemble...\")\n",
    "\n",
    "# Robust Parameters for Multiclass F1\n",
    "lgbm_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_encoded)),\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.015,      # Low rate for better generalization\n",
    "    'num_leaves': 31,            # Standard, prevent overfitting\n",
    "    'max_depth': 8,              # Limit depth to prevent memorization\n",
    "    'min_child_samples': 20,\n",
    "    'feature_fraction': 0.8,     # Randomly select 80% of features per tree\n",
    "    'bagging_fraction': 0.8,     # Randomly select 80% of data per iteration\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 1.0,            # L1 regularization\n",
    "    'lambda_l2': 10.0,           # L2 \n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# 10-Fold CV for maximum stability\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Holders for predictions\n",
    "oof_preds = np.zeros((len(X), len(le.classes_)))\n",
    "test_preds = np.zeros((len(X_test), len(le.classes_)))\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n",
    "    X_train, y_train = X.iloc[train_idx], y_encoded[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # Create dataset\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    \n",
    "    # Train\n",
    "    model = lgb.train(\n",
    "        lgbm_params,\n",
    "        dtrain,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        num_boost_round=3000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=0) # Silence logs\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_prob = model.predict(X_val)\n",
    "    oof_preds[val_idx] = val_prob\n",
    "    \n",
    "    # Score\n",
    "    val_pred_labels = np.argmax(val_prob, axis=1)\n",
    "    fold_score = f1_score(y_val, val_pred_labels, average='macro')\n",
    "    cv_scores.append(fold_score)\n",
    "    \n",
    "    # Test accumulation (average later)\n",
    "    test_preds += model.predict(X_test) / n_folds\n",
    "    \n",
    "    print(f\"Fold {fold+1}/{n_folds} | F1 Macro: {fold_score:.5f}\")\n",
    "\n",
    "print(f\"\\nAverage CV F1: {np.mean(cv_scores):.5f} +/- {np.std(cv_scores):.5f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. SUBMISSION\n",
    "# ==========================================\n",
    "final_pred_indices = np.argmax(test_preds, axis=1)\n",
    "final_pred_labels = le.inverse_transform(final_pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test['participant_id'],\n",
    "    'personality_cluster': final_pred_labels\n",
    "})\n",
    "\n",
    "filename = 'submission_pure_lgbm.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"Submission saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66709a9c-ef03-4913-96ac-434ee262f3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
