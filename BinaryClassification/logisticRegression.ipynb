{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eada8a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "\n",
    "train = pd.read_csv(\"BinaryTrain.csv\")\n",
    "test = pd.read_csv(\"TestBinary.csv\")\n",
    "\n",
    "\n",
    "#  Drop exact duplicates\n",
    "\n",
    "dup_count = train.duplicated().sum()\n",
    "print(\"Duplicate rows found:\", dup_count)\n",
    "\n",
    "if dup_count > 0:\n",
    "    train = train.drop_duplicates()\n",
    "    print(\"Duplicates removed. New shape:\", train.shape)\n",
    "\n",
    "# 2. Split Target + Features\n",
    "TARGET = \"retention_status\"\n",
    "ID_COL = \"founder_id\"\n",
    "\n",
    "y = train[TARGET]\n",
    "X = train.drop(columns=[TARGET, ID_COL])\n",
    "X_test = test.drop(columns=[ID_COL])\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "\n",
    "# 3. Handle Rare Categories in Categorical Columns\n",
    "\n",
    "cat_raw_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "min_count = 50  # you can tweak this\n",
    "\n",
    "for col in cat_raw_cols:\n",
    "    vc = X[col].value_counts()\n",
    "    rare_labels = vc[vc < min_count].index\n",
    "    if len(rare_labels) > 0:\n",
    "        X[col] = X[col].replace(rare_labels, \"Other\")\n",
    "        # apply same mapping to test set\n",
    "        X_test[col] = X_test[col].where(~X_test[col].isin(rare_labels), \"Other\")\n",
    "\n",
    "\n",
    "# 4. Preprocessing with Feature Engineering\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    # polynomial features only on numeric columns\n",
    "    (\"poly\", PolynomialFeatures(\n",
    "        degree=2,\n",
    "        include_bias=False,\n",
    "        interaction_only=True  # only interactions + linear\n",
    "    ))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        drop=\"first\",          # avoid perfect multicollinearity\n",
    "        sparse_output=False    # if using sklearn >= 1.2; else use sparse=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_cols),\n",
    "    (\"cat\", categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# 5. Fit & transform full training and test data\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Processed train shape:\", X_processed.shape)\n",
    "print(\"Processed test shape:\", X_test_processed.shape)\n",
    "\n",
    "# 6. Train/Validation Split (for hyperparameter tuning)\n",
    "\n",
    "X_train_s, X_val_s, y_train_s, y_val_s = train_test_split(\n",
    "    X_processed, y_encoded,\n",
    "    test_size=0.2,\n",
    "    stratify=y_encoded,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 7. Hyperparameter Tuning: C + Threshold\n",
    "C_values = [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 5.0, 10.0]\n",
    "\n",
    "best_f1 = -1\n",
    "best_C = None\n",
    "best_thresh = None\n",
    "\n",
    "for C in C_values:\n",
    "    logreg = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    logreg.fit(X_train_s, y_train_s)\n",
    "    val_probs = logreg.predict_proba(X_val_s)[:, 1]\n",
    "\n",
    "    # threshold search\n",
    "    for thr in np.linspace(0.15, 0.85, 71):\n",
    "        preds = (val_probs >= thr).astype(int)\n",
    "        score = f1_score(y_val_s, preds, average=\"weighted\")\n",
    "\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_C = C\n",
    "            best_thresh = thr\n",
    "\n",
    "print(\"\\n===== BEST HYPERPARAMETERS FOUND =====\")\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "print(f\"Best Validation F1: {best_f1:.4f}\")\n",
    "\n",
    "# Train model with best C to inspect val performance once\n",
    "best_model = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    C=best_C,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_model.fit(X_train_s, y_train_s)\n",
    "val_probs = best_model.predict_proba(X_val_s)[:, 1]\n",
    "val_preds = (val_probs >= best_thresh).astype(int)\n",
    "\n",
    "print(\"\\nValidation Classification Report (with tuned C & threshold):\")\n",
    "print(classification_report(y_val_s, val_preds, target_names=le.classes_))\n",
    "\n",
    "# 8. Train Final Logistic Regression on Full Data\n",
    "\n",
    "logreg_final = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    C=best_C,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining FINAL Logistic Regression on full data...\")\n",
    "logreg_final.fit(X_processed, y_encoded)\n",
    "\n",
    "# 9. Test Predictions with Best Threshold\n",
    "test_probs = logreg_final.predict_proba(X_test_processed)[:, 1]\n",
    "test_preds_encoded = (test_probs >= best_thresh).astype(int)\n",
    "test_preds_labels = le.inverse_transform(test_preds_encoded)\n",
    "\n",
    "\n",
    "# 10. Save Submission File\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"founder_id\": test[\"founder_id\"],\n",
    "    \"retention_status\": test_preds_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_logreg_optimized.csv\", index=False)\n",
    "print(\"\\nLogistic Regression submission saved as submission_logreg_optimized.csv\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
