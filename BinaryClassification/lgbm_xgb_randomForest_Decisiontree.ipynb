{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66b57c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. PREPROCESSING \n",
    "\n",
    "print(\"Loading Data...\")\n",
    "train = pd.read_csv('BinaryTrain.csv')\n",
    "test = pd.read_csv('TestBinary.csv')\n",
    "\n",
    "TARGET = 'retention_status'\n",
    "ID_COL = 'founder_id'\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop(columns=[TARGET, ID_COL])\n",
    "y = train[TARGET]\n",
    "X_test = test.drop(columns=[ID_COL], errors='ignore')\n",
    "\n",
    "# Encode Target (Retained/Exited -> 0/1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Define Preprocessing\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Helper function to Train, Tune, and Save\n",
    "def run_model(name, model, param_dist):\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "\n",
    "    # Create Pipeline\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Fast Hyperparameter Tuning\n",
    "    search = RandomizedSearchCV(\n",
    "        clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,             # 10 random combinations (Fast & Effective)\n",
    "        scoring='f1_weighted', # Kaggle usually likes F1 or Accuracy\n",
    "        cv=3,                  # 3-Fold CV is faster than 5\n",
    "        n_jobs=-1,             # Use all CPU cores\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    search.fit(X, y_encoded)\n",
    "    print(f\"  Best Params: {search.best_params_}\")\n",
    "    print(f\"  Best CV Score: {search.best_score_:.4f}\")\n",
    "\n",
    "    # Predict on Test\n",
    "    best_model = search.best_estimator_\n",
    "    preds_encoded = best_model.predict(X_test)\n",
    "    preds_labels = le.inverse_transform(preds_encoded)\n",
    "\n",
    "    # Save Submission\n",
    "    filename = f\"submission_{name.lower().replace(' ', '_')}.csv\"\n",
    "    sub = pd.DataFrame({\n",
    "        'founder_id': test['founder_id'],\n",
    "        'retention_status': preds_labels\n",
    "    })\n",
    "    sub.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# 2. MODEL DEFINITIONS\n",
    "\n",
    "# 1. Random Forest\n",
    "\n",
    "rf_params = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "run_model('Random Forest', RandomForestClassifier(random_state=42, n_jobs=-1), rf_params)\n",
    "\n",
    "\n",
    "# 2. XGBoost\n",
    "\n",
    "xgb_params = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'model__n_estimators': [100, 300, 500],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "run_model('XGBoost', xgb.XGBClassifier(tree_method='hist', random_state=42, n_jobs=-1), xgb_params)\n",
    "\n",
    "\n",
    "#3. LightGBM\n",
    "\n",
    "lgbm_params = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 300, 500],\n",
    "    'model__num_leaves': [31, 50, 70],\n",
    "    'model__class_weight': ['balanced', None]\n",
    "}\n",
    "run_model('LightGBM', lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1), lgbm_params)\n",
    "\n",
    "\n",
    "# 4. CatBoost\n",
    "\n",
    "cat_params = {\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__depth': [4, 6, 8],\n",
    "    'model__iterations': [200, 500, 800]\n",
    "}\n",
    "\n",
    "run_model('CatBoost', CatBoostClassifier(random_state=42, verbose=0, thread_count=-1), cat_params)\n",
    "\n",
    "\n",
    "# 5. Decision Tree\n",
    "dt_params = {\n",
    "    'model__max_depth': [5, 10, 20, None],\n",
    "    'model__min_samples_leaf': [1, 5, 10, 20],\n",
    "    'model__criterion': ['gini', 'entropy']\n",
    "}\n",
    "run_model('Decision Tree', DecisionTreeClassifier(random_state=42), dt_params)\n",
    "\n",
    "print(\"\\nAll models trained and submissions saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
